# 支持向量机
支持向量机（Support Vector Machines, SVM）是一种二分类模型。它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机；支持向量机还包括核技巧，这使其成为实质上的非线性分类器。

SVM 的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数的最小化问题。

SVM 的最优化算法是求解凸二次规划的最优化算法。

## 什么是支持向量

训练数据集中与分离超平面距离最近的样本点的实例称为支持向量

更通俗的解释：
数据集种的某些点，位置比较特殊。比如 x+y-2=0 这条直线，假设出现在直线上方的样本记为 A 类，下方的记为 B 类。
在寻找找这条直线的时候，一般只需看两类数据，它们各自最靠近划分直线的那些点，而其他的点起不了决定作用。
这些点就是所谓的“支持点”，在数学中，这些点称为向量，所以更正式的名称为“支持向量”。


## 支持向量机的分类
(1) 线性可分支持向量机

当训练数据线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分支持向量机，又称硬间隔支持向量机。

(2) 线性支持向量机

当训练数据接近线性可分时，通过软间隔最大化，学习一个线性分类器，即线性支持向量机，又称软间隔支持向量机。

(3) 非线性支持向量机
当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机。

## 核函数与核技巧

核函数表示将输入从输入空间映射到特征空间后得到的特征向量之间的内积

# 最大间隔超平面背后的原理
相当于在最小化权重时对训练误差进行了约束——对比 L2 范数正则化，则是在最小化训练误差时，对权重进行约束

相当于限制了模型复杂度——在一定程度上防止过拟合，具有更强的泛化能力

## 支持向量机推导

当训练数据线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分支持向量机，又称硬间隔支持向量机。
线性 SVM 的推导分为两部分

1: 如何根据间隔最大化的目标导出 SVM 的标准问题；
2: 拉格朗日乘子法对偶问题的求解过程.









